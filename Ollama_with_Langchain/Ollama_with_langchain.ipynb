{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ollama Models\n",
    "gemma2:2b                                                        \n",
    "llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model='gemma2:2b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoking directly with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Gemma. 😊  How can I help you today? \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('What is your name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoking with messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'French: Bonjour, comment allez-vous ? \\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"Translate the following from English to French\"),\n",
    "    HumanMessage(content=\"Hello How are you?\")\n",
    "]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoking LLM with Prompt and Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, let\\'s dive into the world of LangChain! \\n\\n**What is LangChain?**\\n\\nLangchain is a powerful framework designed to simplify and streamline the development of applications powered by large language models (LLMs). Think of it as the Swiss Army knife for building LLM-based AI solutions.  Here\\'s why it\\'s so popular:\\n\\n* **Abstraction:** LangChain handles complex tasks like managing prompts, retrieving data, and interacting with LLMs. This lets you focus on the core logic of your applications without getting bogged down in low-level details.\\n* **Modular Design:** You can assemble various components like chains (sequences of operations), memory (to store and recall information), and agents (for autonomous decision making). \\n* **Extensible:** LangChain encourages flexibility, letting you easily integrate with different LLMs and data sources (think databases, APIs, etc.).\\n\\n**Why is it so exciting?**\\n\\nLangChain empowers developers to build a wide range of innovative applications like:\\n\\n* **Question Answering Bots:**  Powering chatbots that provide accurate answers.\\n* **Text Summarization Tools:** Creating concise summaries of long documents. \\n* **Data Extraction Systems:** Extracting relevant information from various sources (like customer emails or legal documents).\\n* **Code Generation Assistants:** Helping programmers write more efficient and error-free code. \\n\\n**Key Features:**\\n\\n1. **Prompts and Chains:** LangChain excels at crafting the right prompts for your LLM and chaining multiple steps together to achieve complex tasks. You can build chains of actions like: \"Summarize this document, then translate it into Spanish.\"\\n2. **Memory Management:**  To ensure context is retained throughout an application\\'s flow. Imagine a chatbot remembering past conversations so it can provide more personalized responses over time. \\n3. **Agents:** LangChain allows you to create autonomous agents that can make decisions based on their understanding of the world (e.g., an agent could book travel arrangements or send emails).\\n\\n**Getting Started with LangChain:**\\n\\nLangChain is open-source and has a thriving community. Here\\'s where to find resources:\\n* **Website:**  [https://langchain.com/](https://langchain.com/) \\n* **Documentation:** [https://python.langchain.com/en/latest/index.html](https://python.langchain.com/en/latest/index.html) \\n* **GitHub Repository:** [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain)\\n\\n**In summary,** LangChain is a game-changer for building more sophisticated and impactful LLM-driven applications. Its modularity, flexibility, and powerful features are driving innovation in the field of AI! \\n\\n\\n\\nLet me know if you have any specific questions or would like to explore some use cases in detail!  \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','You are an expert AI Engineer. Provide me answers based on the questions'),\n",
    "        ('user','{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|llm\n",
    "response = chain.invoke({'input':'Tell me about Langsimth?'})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining with output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Gans (Generative Adversarial Networks)** are a type of deep learning model that combines two neural networks, a **generator** and a **discriminator**, to create realistic synthetic data. \\n\\nHere\\'s how they work:\\n\\n* **Generator:** This network takes random noise as input and attempts to create an output resembling real-world data like images, videos, or text.\\n* **Discriminator:** This network receives both real and generated data (from the generator) and tries to differentiate between them. The discriminator assigns a score based on how well the generated data looks similar to real data. \\n\\nThe training process:\\n\\n1. **Training the Discriminator:**  The discriminator is trained to label each input as either real or fake. Through backpropagation, it learns to identify subtle differences between genuine and synthetic data.\\n2. **Training the Generator:** The generator tries to create data that closely mimics what the discriminator labels as \"real\". It receives feedback from the discriminator through a loss function (often called \"GAN Loss\"). This feedback helps the generator improve its data-generation capabilities. \\n\\n**Key Features of GANs:**\\n\\n* **Generative ability:** GANs can generate new, realistic samples similar to real world data.\\n* **Learning process:** They learn how to mimic specific data distributions by learning from their own generated outputs and discriminator feedback. \\n* **Flexibility:**  GANs can be applied to a variety of domains including image generation (like photos or paintings), text-to-image synthesis, audio generation, video creation, and more.\\n\\n**Applications of GANs:**\\n\\n* **Image Generation:** Creating realistic images for art, advertising, or design. \\n* **Data Augmentation:** Generating artificial data to improve training datasets in machine learning models like image recognition systems.\\n* **Text-to-Image Synthesis:**  Creating images based on textual descriptions. \\n* **Drug Discovery:** Generating potential drug molecules with desired properties.\\n\\n\\n**Challenges of GANs:**\\n\\n* **Mode Collapse:**  The generator may get stuck creating only a limited range of outputs, limiting the diversity of generated data (e.g., all images might look similar to each other).\\n* **Training Instability:**  GANs are sensitive to training hyperparameters and require careful tuning for successful training. \\n* **Ethical Considerations:** There\\'s potential for misuse of GANs for creating fake content like deepfakes, raising ethical concerns about their impact on society.\\n\\n\\n\\nLet me know if you have more questions! \\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "response= chain.invoke('What are Gans?')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# from langchain_community.llms import Ollama\n",
    "# import streamlit as st\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# ## Langsmith Tracking\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "# ## Prompt Template\n",
    "# prompt=ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\",\"You are a helpful assistant. Please respond to the question asked\"),\n",
    "#         (\"user\",\"Question:{question}\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# ## streamlit framework\n",
    "# st.title(\"Langchain Demo With Gemma Model\")\n",
    "# input_text=st.text_input(\"What question you have in mind?\")\n",
    "\n",
    "\n",
    "# ## Ollama Llama2 model\n",
    "# llm=Ollama(model=\"gemma:2b\")\n",
    "# output_parser=StrOutputParser()\n",
    "# chain=prompt|llm|output_parser\n",
    "\n",
    "# if input_text:\n",
    "#     st.write(chain.invoke({\"question\":input_text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains and Invoking by giving our custom content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_data = WebBaseLoader('https://docs.smith.langchain.com/tutorials/Administrators/manage_spend').load()\n",
    "split = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "split_web_data = split.split_documents(web_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='gemma2:2b')\n",
    "vect_embed_data = FAISS.from_documents(split_web_data,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrival Chain, Document Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "# document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What are LangSmith's two usage limits?  \\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    'input':\"What are LangSmith's two usage limits?\",\n",
    "    'context':[Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph. \")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriever = vect_embed_data.as_retriever()\n",
    "ret_chain = create_retrieval_chain(retriever,document_chain)\n",
    "response=ret_chain.invoke({\"input\":\"LangSmith has two usage limits: total traces and extended\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangSmith has two usage limits: total traces and extended',\n",
       " 'context': [Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='meaning we log around 100,000-130,000 traces per dayExpected Growth in Load: We expect to double in size in the near future.From these assumptions, we can do a quick back-of-the-envelope calculation to get a good limit of:limit = current_load_per_day * expected_growth * days/month      = 130,000 * 2 * 30      = 7,800,000 traces / monthWe click on the edit icon on the right side of the table for our Prod row, and can enter this limit as follows:noteWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.Cutting maximum spend with an extended data retention limit\\u200bIf we are not a big enterprise, we may shudder at the ~$40k per month bill.We saw from Optimization 1 that the easiest way to cut cost was through managing data retention.\\nThe same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum'),\n",
       "  Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='want to be more liberal with your usage limits to avoid test failures.Now that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:With this estimator, we can be confident that we will not end up with an unexpected credit card bill at the end of the month.Summary\\u200bIn this tutorial, we learned how to:Cut down our existing costs with data retention policiesPrevent future overspend with usage limitsIf you have questions about further optimizing your spend, please reach out to support@langchain.dev.Was this page helpful?You can leave detailed feedback on GitHub.PreviousTutorialsNextAdd observability to your LLM applicationProblem SetupUnderstand your current usageUsage GraphInvoicesOptimization 1: manage data retentionChange org level retention defaults for new projectsChange project level retention defaultsKeep around a percentage of traces for extended data retentionSee results after 7 daysOptimization 2: limit usageSetting a good total'),\n",
       "  Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='The same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum\\nhigh retention traces we can keep. That would be .10 * 7,800,000 = 780,000.As we can see, the maximum cost is cut from ~40k per month to ~7.5k per month, because we no longer allow as many expensive\\ndata retention upgrades. This lets us be confident that new users on the platform will not accidentally cause cost to balloon.noteThe extended data retention limit can cause features other than traces to stop working once reached. If you plan to\\nuse this feature, please read more about its functionality here.Set dev/staging limits and view total spent limit across workspaces\\u200bFollowing the same logic for our dev and staging environments, whe set limits at 10% of the production\\nlimit on usage for each workspace.While this works with our usage pattern, setting good dev and staging limits may vary depending on'),\n",
       "  Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='clearly think about your assumptions before setting a limit.For example:Current Load: Our gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it,')],\n",
       " 'answer': \"The provided context outlines a workflow for managing data retention and usage limits in the context of an AI application, specifically LangSmith.  Here's a breakdown:\\n\\n**Understanding Costs & Limits:**\\n\\n* **High Usage Costs:** The initial cost estimate for extended data retention is $40,000 per month, which suggests that data retention features can be expensive. \\n* **Data Retention Strategy:**  They utilize an 'extended data retention' strategy where they keep around 10% of traces older than 14 days for analysis and historical use. This limits costs significantly, as the full dataset is not retained.\\n* **Usage Limits:** They implemented usage limits to prevent unexpected spending. A limit of ~7.5k per month was achievable by keeping only ~10% of traces (representing a lower volume than the potential cost) for extended retention.\\n\\n**How to Effectively Set Usage Limits:**\\n\\n* **Assumptions:** It's crucial to clearly define assumptions about usage patterns, like their AI application's request frequency and associated trace generation.  \\n* **Example:** The context notes that requests are made between 1.2-1.5 times per second, leading to an estimated ~780,000 traces that require extended retention.\\n\\n\\n**Next Steps:**\\n\\n* **Test Environments:**  The usage limits were implemented for dev and staging environments with a 10% limit based on the production environment.\\n* **Planning & Strategy:** This example suggests that a clear plan for usage needs to be established before setting limits. \\n\\n\\n\\n**Key Takeaways:**\\n\\n* **Minimize Costs:** Data retention limits are crucial for preventing unexpected costs associated with data storage.  \\n* **Strategic Usage Limits:** Well-defined usage limits help manage resource utilization and optimize spending, especially when dealing with expensive features like extended data retention.\\n* **Assumption Evaluation:** The context emphasizes the need to analyze current usage patterns and make informed decisions about future limits based on predicted growth and anticipated workloads. \\n\\n\\n\\nLet me know if you have any other questions or would like more detail on specific points! \\n\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The provided context outlines a workflow for managing data retention and usage limits in the context of an AI application, specifically LangSmith.  Here's a breakdown:\\n\\n**Understanding Costs & Limits:**\\n\\n* **High Usage Costs:** The initial cost estimate for extended data retention is $40,000 per month, which suggests that data retention features can be expensive. \\n* **Data Retention Strategy:**  They utilize an 'extended data retention' strategy where they keep around 10% of traces older than 14 days for analysis and historical use. This limits costs significantly, as the full dataset is not retained.\\n* **Usage Limits:** They implemented usage limits to prevent unexpected spending. A limit of ~7.5k per month was achievable by keeping only ~10% of traces (representing a lower volume than the potential cost) for extended retention.\\n\\n**How to Effectively Set Usage Limits:**\\n\\n* **Assumptions:** It's crucial to clearly define assumptions about usage patterns, like their AI application's request frequency and associated trace generation.  \\n* **Example:** The context notes that requests are made between 1.2-1.5 times per second, leading to an estimated ~780,000 traces that require extended retention.\\n\\n\\n**Next Steps:**\\n\\n* **Test Environments:**  The usage limits were implemented for dev and staging environments with a 10% limit based on the production environment.\\n* **Planning & Strategy:** This example suggests that a clear plan for usage needs to be established before setting limits. \\n\\n\\n\\n**Key Takeaways:**\\n\\n* **Minimize Costs:** Data retention limits are crucial for preventing unexpected costs associated with data storage.  \\n* **Strategic Usage Limits:** Well-defined usage limits help manage resource utilization and optimize spending, especially when dealing with expensive features like extended data retention.\\n* **Assumption Evaluation:** The context emphasizes the need to analyze current usage patterns and make informed decisions about future limits based on predicted growth and anticipated workloads. \\n\\n\\n\\nLet me know if you have any other questions or would like more detail on specific points! \\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
